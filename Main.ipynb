{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_32(x):\n",
    "    return x.type(torch.FloatTensor)\n",
    "\n",
    "def to_64(x):\n",
    "    return x.type(torch.DoubleTensor)\n",
    "\n",
    "def pos_gram(gram, regularlizer = None):\n",
    "    _type = gram.type()\n",
    "    _size = len(gram)\n",
    "    if regularlizer is None:\n",
    "        if gram.abs().max() == 0:\n",
    "            raise ValueError(\"gram error, expect matrix with none-zero element\")\n",
    "        \n",
    "        # the fraction of float32 is 2**(-23)~10**(-7) we start with 10**(-7) times of maximun element\n",
    "        regularlizer = gram.abs().max()*0.0000001\n",
    "    \n",
    "    if regularlizer <= 0:\n",
    "        raise ValueError(\"regularlizer error, expect positive, got %s\" %(regularlizer))\n",
    "    \n",
    "    while True:\n",
    "        lambdas, vectors = torch.symeig(gram + regularlizer*torch.eye(_size).type(_type))\n",
    "        if lambdas.min() > 0:\n",
    "            break\n",
    "        \n",
    "        regularlizer *= 2.\n",
    "    \n",
    "    return gram + regularlizer*torch.eye(_size).type(_type)\n",
    "\n",
    "class LinearExpander():\n",
    "    def __init__(self, linear_model, activation_function, candidate_num=1, std = None):\n",
    "        self.linear_model = linear_model\n",
    "        self.activation_function = activation_function\n",
    "        self.candidate = torch.nn.Linear(self.linear_model.in_features, candidate_num)\n",
    "        if std is not None:\n",
    "            self.candidate.weight.data *= torch.tensor(std*(3*self.candidate.in_features)**0.5)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # regressor_gram : store X^t*X\n",
    "        # projector : store X^t*Y\n",
    "        # responsor_ss : store component-wise square sum of y (=diag(Y^t*Y))\n",
    "        self.regressor_gram = torch.zeros((self.linear_model.out_features+1, self.linear_model.out_features+1))\n",
    "        self.projector = torch.zeros((self.linear_model.out_features+1, self.candidate.out_features))\n",
    "        self.responsor_ss = torch.zeros((self.candidate.out_features))\n",
    "        self.datums_acc = 0\n",
    "    \n",
    "    def data_input(self, data):\n",
    "        datums = data.size()[0]\n",
    "        regressor = self.linear_model(data)\n",
    "        regressor = self.activation_function(regressor)\n",
    "        expand = torch.cat((regressor, torch.ones((datums, 1))), 1)\n",
    "        self.regressor_gram += torch.mm(expand.t(), expand)\n",
    "        \n",
    "        responsor = self.candidate(data)\n",
    "        responsor = self.activation_function(responsor)\n",
    "        self.projector += torch.mm(expand.t(), responsor)\n",
    "        \n",
    "        self.responsor_ss += (responsor**2).sum(0)\n",
    "        \n",
    "        self.datums_acc += datums\n",
    "    \n",
    "    def take(self, take_num=1, choice_round=0):\n",
    "        lots_num = self.candidate.out_features\n",
    "        if take_num > lots_num:\n",
    "            raise ValueError(\"take_num exceed candidate\")\n",
    "        \n",
    "        if (choice_round == 0):\n",
    "            took_idx = torch.randperm(lots_num)[:take_num]\n",
    "        else:\n",
    "            mean_gram = pos_gram(to_64(self.regressor_gram / self.datums_acc)) # avoid singular gram\n",
    "            lambdas, vectors = torch.symeig(mean_gram, eigenvectors=True) # eigen\n",
    "            mean_projector = to_64(self.projector/self.datums_acc)\n",
    "            lambdas_inv = 1/(lambdas+0.0000001) \n",
    "            VtXtY = vectors.t().mm(mean_projector)\n",
    "            dependency = ((VtXtY.t()*lambdas_inv).t()*VtXtY).sum(0) # diag of Y^t*X*Gram^(-1)*X^t*Y\n",
    "            independency = to_64(self.responsor_ss/self.datums_acc) - dependency\n",
    "            took_idx = []\n",
    "            for t in range(take_num):\n",
    "                hit = np.random.randint(self.candidate.out_features)\n",
    "                while hit in took_idx:\n",
    "                    hit = np.random.randint(self.candidate.out_features)\n",
    "                \n",
    "                for c in range(choice_round):\n",
    "                    new_hit = np.random.randint(self.candidate.out_features)\n",
    "                    while new_hit in took_idx:\n",
    "                        new_hit = np.random.randint(self.candidate.out_features)\n",
    "                    \n",
    "                    if np.random.rand() < (independency[new_hit]/independency[hit]):\n",
    "                        hit = new_hit\n",
    "                \n",
    "                took_idx.append(hit)\n",
    "            \n",
    "        return took_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = torch.nn.Linear(20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myLE = LinearExpander(foo, torch.tanh, candidate_num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.random.normal(0,1,(1000,20))\n",
    "myLE.data_input(torch.FloatTensor(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = torch.zeros((20))\n",
    "for t in range(10000):\n",
    "    counter[myLE.take(1, choice_round=10)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([286., 676., 464., 579., 645., 596., 577., 364., 532., 645., 610., 453.,\n",
       "        426., 393., 371., 470., 469., 519., 409., 516.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor = torch.FloatTensor(data)\n",
    "regressor = myLE.linear_model(regressor)\n",
    "regressor = myLE.activation_function(regressor)\n",
    "regressor = np.array(regressor.data)\n",
    "regressor = np.concatenate((regressor, np.ones((len(regressor), 1))), axis=1)\n",
    "\n",
    "responsor = torch.FloatTensor(data)\n",
    "responsor = myLE.candidate(responsor)\n",
    "responsor = myLE.activation_function(responsor)\n",
    "responsor = np.array(responsor.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5.16156042, 4.63571527, 4.96884496, 4.81084715, 4.57201779,\n",
       "       4.4853007 , 4.69594333, 4.3789729 , 4.38589555, 4.47402833,\n",
       "       4.46076148, 4.77183801, 4.49550764, 4.4203575 , 4.81240436,\n",
       "       4.43681148, 4.639682  , 4.74345333, 4.23209378, 4.31664408])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(counter)/np.linalg.lstsq(regressor, responsor)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
