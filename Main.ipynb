{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_32(x):\n",
    "    return x.type(torch.FloatTensor)\n",
    "\n",
    "def to_64(x):\n",
    "    return x.type(torch.DoubleTensor)\n",
    "\n",
    "def pos_gram(gram, regularlizer = None):\n",
    "    _type = gram.type()\n",
    "    _size = len(gram)\n",
    "    if regularlizer is None:\n",
    "        if gram.abs().max() == 0:\n",
    "            raise ValueError(\"gram error, expect matrix with none-zero element\")\n",
    "        \n",
    "        # the fraction of float32 is 2**(-23)~10**(-7) we start with 10**(-7) times of maximun element\n",
    "        regularlizer = gram.abs().max()*0.0000001\n",
    "    \n",
    "    if regularlizer <= 0:\n",
    "        raise ValueError(\"regularlizer error, expect positive, got %s\" %(regularlizer))\n",
    "    \n",
    "    while True:\n",
    "        lambdas, vectors = torch.symeig(gram + regularlizer*torch.eye(_size).type(_type))\n",
    "        if lambdas.min() > 0:\n",
    "            break\n",
    "        \n",
    "        regularlizer *= 2.\n",
    "    \n",
    "    return gram + regularlizer*torch.eye(_size).type(_type)\n",
    "\n",
    "class LinearExpander():\n",
    "    def __init__(self, linear_model, activation_function, candidate_num=1, std = None):\n",
    "        self.linear_model = linear_model\n",
    "        self.activation_function = activation_function\n",
    "        self.candidate = torch.nn.Linear(self.linear_model.in_features, candidate_num)\n",
    "        if std is not None:\n",
    "            self.candidate.weight.data *= torch.tensor(std*(3*self.candidate.in_features)**0.5)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # regressor_gram : store X^t*X\n",
    "        # projector : store X^t*Y\n",
    "        # responsor_ss : store component-wise square sum of y (=diag(Y^t*Y))\n",
    "        self.regressor_gram = torch.zeros((self.linear_model.out_features+1, self.linear_model.out_features+1)).data\n",
    "        self.projector = torch.zeros((self.linear_model.out_features+1, self.candidate.out_features)).data\n",
    "        self.responsor_ss = torch.zeros((self.candidate.out_features)).data\n",
    "        self.datums_acc = 0\n",
    "    \n",
    "    def data_input(self, data):\n",
    "        datums = data.size()[0]\n",
    "        regressor = self.linear_model(data).data\n",
    "        regressor = self.activation_function(regressor)\n",
    "        expand = torch.cat((regressor, torch.ones((datums, 1))), 1)\n",
    "        self.regressor_gram += torch.mm(expand.t(), expand)\n",
    "        responsor = self.candidate(data).data\n",
    "        responsor = self.activation_function(responsor)\n",
    "        self.projector += torch.mm(expand.t(), responsor)\n",
    "        self.responsor_ss += (responsor**2).sum(0)\n",
    "        self.datums_acc += datums\n",
    "    \n",
    "    def take(self, take_num=1, weighted=True):\n",
    "        # return index of hitted candidate\n",
    "        lots_num = self.candidate.out_features\n",
    "        if take_num > lots_num:\n",
    "            raise ValueError(\"take_num exceed candidate\")\n",
    "        \n",
    "        if weighted:\n",
    "            if self.datums_acc == 0:\n",
    "                raise ZeroDivisionError(\"input data before take(with weighted)\")\n",
    "            mean_gram = pos_gram(to_64(self.regressor_gram / self.datums_acc)) # avoid singular gram\n",
    "            lambdas, vectors = torch.symeig(mean_gram, eigenvectors=True) # eigen\n",
    "            mean_projector = to_64(self.projector/self.datums_acc)\n",
    "            lambdas_inv = 1/(lambdas+0.0000001) \n",
    "            VtXtY = vectors.t().mm(mean_projector)\n",
    "            dependency = ((VtXtY.t()*lambdas_inv).t()*VtXtY).sum(0) # diag of Y^t*X*Gram^(-1)*X^t*Y\n",
    "            independency = to_64(self.responsor_ss/self.datums_acc) - dependency\n",
    "            prob = independency/independency.sum()\n",
    "            output = np.random.choice(self.candidate.out_features, take_num, replace=False, p=prob)\n",
    "        else:\n",
    "            output = torch.randperm(lots_num)[:take_num]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def expand(self, expand_size=1, weighted=True):\n",
    "        # return new linear model by self.take\n",
    "        ori_in = self.linear_model.in_features\n",
    "        ori_out = self.linear_model.out_features\n",
    "        output = torch.nn.Linear(ori_in, ori_out + expand_size)\n",
    "        take_idx = self.take(expand_size, weighted)\n",
    "        output.weight.data[:ori_out] = self.linear_model.weight.data\n",
    "        output.weight.data[ori_out:] = self.candidate.weight.data[take_idx]\n",
    "        output.bias.data[:ori_out] = self.linear_model.bias.data\n",
    "        output.bias.data[ori_out:] = self.candidate.bias.data[take_idx]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheak weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1234., 1100.,  785., 1228., 1452., 1187., 1439.,  871., 1526., 1768.,\n",
       "        1912., 1195., 1086.,  794., 1747., 2153.,  641., 1159.,  522., 1213.,\n",
       "        1207., 1015., 1312., 1788., 1824., 1107.,  626.,  666., 1425., 1326.,\n",
       "        1517., 1572.,  733., 1633., 1175., 1350.,  698., 1463., 1668.,  883.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = torch.nn.Linear(20,10)\n",
    "myLE = LinearExpander(foo, torch.tanh, candidate_num=40)\n",
    "data = np.random.normal(0,1,(1000,20))\n",
    "myLE.data_input(torch.FloatTensor(data))\n",
    "counter = torch.zeros((40))\n",
    "for t in range(10000):\n",
    "    counter[myLE.take(5, True)] += 1\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([11.59787722, 11.56528757, 11.18942885, 11.95476503, 11.32864264,\n",
       "       11.87806955, 11.58442249, 11.32172124, 11.83319704, 11.39082338,\n",
       "       12.10086202, 11.31275669, 12.43541815, 12.00051774, 12.13118769,\n",
       "       11.97519754, 11.40217909, 12.14422874, 11.79643811, 11.64154495,\n",
       "       11.83689666, 11.49320803, 12.47963341, 11.64668437, 12.00349998,\n",
       "       11.13810758, 12.82068284, 11.97046085, 11.67075666, 12.5716104 ,\n",
       "       11.40071838, 11.740976  , 11.4707785 , 11.54096596, 11.367652  ,\n",
       "       11.55207985, 10.83640692, 11.75895113, 11.42707201, 11.66830375])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = torch.FloatTensor(data)\n",
    "regressor = myLE.linear_model(regressor)\n",
    "regressor = myLE.activation_function(regressor)\n",
    "regressor = np.array(regressor.data)\n",
    "regressor = np.concatenate((regressor, np.ones((len(regressor), 1))), axis=1)\n",
    "\n",
    "responsor = torch.FloatTensor(data)\n",
    "responsor = myLE.candidate(responsor)\n",
    "responsor = myLE.activation_function(responsor)\n",
    "responsor = np.array(responsor.data)\n",
    "\n",
    "np.array(counter)/np.linalg.lstsq(regressor, responsor)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
